# CH1 Introduction



### **1.1 Heterogeneous Parallel Computing (异构并行计算)**

**Q1: 为什么自 2003 年以后，CPU 的主频提升放缓了？**

- **A:** 因为遇到了**“功耗墙” (Power Wall)**。能耗和散热问题限制了时钟频率的进一步提升。为了继续提升性能，厂商转向了多核（Multicore）设计，而非单纯提高单核频率。

**Q2: CPU 和 GPU 的设计哲学（Design Philosophy）有何本质区别？（必考点）**

- **A:**
  - **CPU (延迟导向 Latency-oriented):** 拥有复杂的控制逻辑（Control）和大容量缓存（Cache），目的是最小化**单个线程**的执行延迟，让串行代码跑得飞快。
  - **GPU (吞吐量导向 Throughput-oriented):** 拥有海量的计算单元（ALU），但控制和缓存较小。目的是通过同时运行成千上万个线程，利用**并行度**来掩盖延迟，最大化单位时间的**总运算量**。

**Q3: 为什么 CUDA 比早期的 GPGPU 成功？**

- **A:** 早期 GPGPU 需要通过图形 API（OpenGL/DirectX）把计算伪装成像素渲染，编程难度极高。CUDA 提供了通用的并行编程接口，且 NVIDIA 显卡拥有巨大的**用户基数 (Installed Base)**，这为开发者创造了良性的市场循环。

------

### **1.2 Why More Speed or Parallelism? (为什么需要更快的速度)**

**Q4: 什么是“数据并行” (Data Parallelism)？它能带来多大的加速？**

- **A:** 数据并行是指对大量不同的数据元素执行相同的操作。如果应用具备良好的数据并行性，GPU 通常只需数小时的开发就能实现 **100倍甚至 1000倍** 的加速。

**Q5: 除了计算速度，还有什么因素限制了并行程序的性能？**

- **A:** **数据管理 (Data Management)**。有效地将数据搬运到计算核心是关键，因为很多时候瓶颈不在于算得不够快，而在于数据传输不够快（Memory Wall）。

------

### **1.3 Speeding Up Real Applications (加速真实应用)**

**Q6: 请解释 Amdahl 定律 (Amdahl's Law) 及其对并行计算的启示。**

- **A:**
  - **定义:** 程序的整体加速比受限于**不可并行化（串行）**部分的比例。
  - **例子:** 如果程序有 30% 必须串行，即使并行部分加速无数倍，总加速比也无法超过 $1/0.3 \approx 3.3$ 倍。
  - **启示:** 要想获得 100 倍加速，并行部分必须占据程序执行时间的 **99% 以上**。

**Q7: 用“桃子理论”解释异构计算的分工。**

- **A:**
  - **桃核 (Pit):** 串行部分（逻辑控制、I/O），很难并行，适合交给 **CPU** 处理。
  - **果肉 (Flesh):** 并行部分（大量重复计算），容易并行，适合交给 **GPU** 处理。
  - CUDA 的目标是扩大“果肉”的覆盖范围。

------

### **1.4 Challenges in Parallel Programming (并行编程的挑战)**

**Q8: 并行编程面临哪四大主要挑战？**

- **A:**
  1. **工作效率 (Work Efficiency):** 并行算法可能会比串行算法做更多的总运算量（例如引入了额外的辅助计算），如果并行度不够，反而会变慢。
  2. **内存瓶颈 (Memory Bound):** 许多应用受限于内存带宽（数据搬运速度）而非计算速度。
  3. **数据分布特征 (Data Characteristics):** 不规则的数据大小或分布会导致**负载不均衡**，有的线程累死，有的线程闲死。
  4. **同步开销 (Synchronization Overhead):** 线程间的协作（如等待 Barrier、原子操作）会带来额外的时间消耗。

------

### **1.5 Related Parallel Programming Interfaces (相关编程接口)**

**Q9: 对比 OpenMP, MPI 和 CUDA/OpenCL 的应用场景。**

- **A:**
  - **OpenMP:** 适用于**共享内存**系统（如多核 CPU），依靠编译器指令自动化并行，简单但对底层控制较弱。
  - **MPI:** 适用于**分布式内存**系统（如集群 Cluster），依靠显式的**消息传递**来交换数据。
  - **CUDA/OpenCL:** 适用于**异构计算**（CPU+GPU），允许程序员**显式控制**数据搬运和线程行为，能挖掘最大性能。

**Q10: 既然 OpenCL 是通用的，为什么还要学 CUDA？**

- **A:** 它们的核心概念（Core Concepts）高度相似。CUDA 是学习并行原理极好的载体，学会了 CUDA，转换到 OpenCL 几乎没有门槛，且 CUDA 的生态和工具目前更为成熟。

------

### **1.6 Overarching Goals (总体目标)**

**Q11: 本书强调的“计算思维” (Computational Thinking) 是指什么？**

- **A:** 指以适应大规模并行硬件的方式来思考和拆解问题的能力，而不仅仅是把传统数学公式照搬进代码。

**Q12: 如何保证并行代码在未来硬件上的“可扩展性” (Scalability)？**

- **A:** 关键在于**规范化 (Regularize)** 和 **本地化 (Localize)** 内存访问。如果数据访问主要发生在核心内部（本地），那么未来的显卡核心越多，程序就会自动跑得越快。

------

### **1.7 Organization of the Book (全书结构)**

**Q13: 本书为什么按照“算法模式” (Patterns) 来组织，而不是按照硬件特性？**

- **A:** 因为通过具体的应用场景（如卷积、直方图）来引入硬件特性（如常量内存、原子操作），比干讲硬件理论更容易理解，也更实用。