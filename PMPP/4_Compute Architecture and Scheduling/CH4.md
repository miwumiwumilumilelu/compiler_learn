# CH4 Compute Architecture and Scheduling



### 4.1 Architecture of a Modern GPU (现代 GPU 架构)

**Q1: CPU 和 GPU 在设计理念上最大的区别是什么？**

> **A:**
>
> - **CPU:** 设计目标是 **最小化延迟 (Minimize Latency)**。它有巨大的缓存和复杂的控制逻辑（分支预测、乱序执行），为了让单个任务跑得尽可能快。
> - **GPU:** 设计目标是 **最大化吞吐量 (Maximize Throughput)**。它通过海量的简单计算核心并行工作，牺牲单线程的延迟来换取整体数据处理的效率。

**Q2: 请简述 GPU 的层级结构 (GPU -> SM -> Core)。**

> **A:**
>
> - **GPU:** 由一组 **SM (Streaming Multiprocessors, 流多处理器)** 组成的阵列。
> - **SM:** GPU 的核心构建模块。SM 内部包含了多个 Core，且这些 Core **共享**同一套控制逻辑（Control Logic）和片上内存（On-chip Memory/Shared Memory）。
> - **Core (CUDA Core/SP):** 最小的执行单元，负责具体的数学运算（浮点/整数计算）。

------

### 4.2 Block Scheduling (线程块调度)

**Q3: CUDA 运行时系统是如何将 Grid 中的 Blocks 分配给 SM 的？**

> **A:** 采用 **Block-by-Block (按块分配)** 的原则。
>
> - 一个 Block 被视为一个**原子单位**，其内部的所有线程必须被**同时**分配给同一个 SM。
> - 一个 SM 可以同时容纳多个 Block（只要资源允许），但一个 Block 不能被拆分到两个 SM 上。

**Q4: 为什么不同 Block 之间的线程无法安全地进行同步？**

> **A:** 因为 Block 的调度顺序是不确定的。 运行时系统维护一个 Block 列表，当 SM 上的旧 Block 执行完释放资源后，才会分配新 Block。如果你让 Block A 等待 Block B，但 Block B 还在排队没被分配，就会导致死锁。这种互不依赖性带来了 **透明的可扩展性**。

------

### 4.3 Synchronization (同步与透明可扩展性)

**Q5: `__syncthreads()` 的作用是什么？它有什么使用限制？**

> **A:**
>
> - **作用:** 它是一个 **栅栏 (Barrier)**。Block 内的所有线程必须都执行到这一行，才能继续往下执行。用于协调 Block 内线程的进度。
> - **限制:** **死锁陷阱**。决不能将它放在只有部分线程能进入的 `if-else` 分支中。如果线程 A 进了 `if` 等待，线程 B 进了 `else` 等待，它们永远等不到对方，导致程序卡死。

**Q6: 什么是“透明的可扩展性” (Transparent Scalability)？**

> **A:** 指同一份 CUDA 代码可以在不同性能的 GPU 上运行而无需修改。
>
> - **硬件弱 (SM 少):** 一次只跑少量 Block，分多批跑完。
> - **硬件强 (SM 多):** 一次并行跑大量 Block，瞬间跑完。 这种能力源于 Block 之间没有相互依赖。

------

### 4.4 Warps and SIMD Hardware (线程束与 SIMD)

**Q7: 什么是 Warp？它和 Block 有什么关系？**

> **A:**
>
> - **定义:** Warp 是 SM 硬件调度的**最小单位**。
> - **大小:** 固定为 **32 个线程**。
> - **关系:** Block 只是软件上的逻辑分组。当 Block 被分配给 SM 后，它会被物理切分为多个 Warp（例如 256 线程的 Block 会被切分为 8 个 Warp）。

**Q8: 对于多维 (2D/3D) Block，硬件是如何将其划分为 Warp 的？**

> **A:** 遵循 **行主序 (Row-Major)** 线性化原则。
>
> - 先排 `x` 维度，再排 `y`，最后排 `z`。
> - 将多维坐标拉成一条直线后，每 32 个线程切一刀组成一个 Warp。
> - 如果 Block 总线程数不是 32 的倍数，最后一个 Warp 会填充 **无效线程 (Inactive Threads/Padding)**。

**Q9: 什么是 SIMD 执行模型？为什么 GPU 要采用这种设计？**

> **A:**
>
> - **SIMD (Single-Instruction, Multiple-Data):** 同一个 Warp 内的 32 个线程在同一时刻执行**同一条指令**，但操作不同的数据。
> - **优势:** **节省芯片面积**。多个 Core 可以共享同一个指令获取/分发单元（Instruction Fetch/Dispatch Unit），从而腾出更多空间给计算单元 (ALU)，大幅提升吞吐量。

------

### 4.5 Control Divergence (控制流发散)

**Q10: 什么是 Control Divergence？它是如何发生的？**

> **A:** 当同一个 Warp 内的线程因为条件分支（如 `if-else`），导致一部分线程想走路径 A，另一部分想走路径 B 时，就发生了发散。
>
> - 因为 SIMD 硬件一次只能发一条指令，不能同时发 A 和 B 的指令。

**Q11: GPU 硬件是如何处理发散的？这会带来什么性能影响？**

> **A:**
>
> - **机制:** **串行回放 (Multipass Approach)**。硬件会先让走路径 A 的线程执行（此时走 B 的线程被强制静默/Inactive），然后再让走路径 B 的线程执行（此时走 A 的静默）。
> - **影响:** 执行时间变成了 A路径时间 + B路径时间。硬件利用率下降。

**Q12: 为什么说处理边界条件的 `if (i < n)` 对性能影响很小？**

> **A:** 因为对于大规模数据，只有**最后一个 Warp**（处理边界的那一组线程）会发生发散。前面成千上万个 Warp 都是全员 `True`，满负荷运行。整体来看，发散带来的损耗通常小于 1-3%。

------

### 4.6 Latency Tolerance (延迟掩盖)

**Q13: 什么是“延迟掩盖” (Latency Hiding)？**

> **A:** 指 GPU 利用大量的线程来填补内存读取等长延迟操作带来的空窗期。 当 Warp A 去读取全局内存（需要几百个周期）时，SM 立刻切换去执行 Warp B。只要 Warp 足够多，计算核心就永远不会闲着。

**Q14: 为什么 GPU 需要“过额订阅” (Oversubscription)？**

> **A:** 每个 SM 只有有限的计算核心（如 64 个），但我们可以分配给它远超核心数的线程（如 2048 个）。 这种 32:1 的比例正是为了保证总有“准备好”的 Warp 可以被调度，从而彻底掩盖内存延迟。

**Q15: 为什么 GPU 切换线程是“零开销” (Zero-overhead) 的？**

> **A:** 因为 GPU 拥有巨大的寄存器文件（Register File）。所有驻留线程的变量都实实在在地存在片上寄存器里。切换 Warp 只是逻辑指针的变动，不需要像 CPU 那样进行昂贵的内存保存/恢复操作。

------

### 4.7 Occupancy (资源划分与占用率)

**Q16: 什么是 Occupancy (占用率)？为什么它很重要？**

> **A:**
>
> - **定义:** `SM上实际活跃的 Warp 数 / SM 支持的最大 Warp 数`。
> - **重要性:** 占用率越高，意味着可用于掩盖延迟的候选 Warp 越多，通常能带来更好的性能。

**Q17: 限制 Occupancy 达到 100% 的主要瓶颈有哪些？**

> **A:**
>
> 1. **Block 数量限制:** Block 太小，导致触碰到“最大 Block 数”上限时，线程槽还没填满。
> 2. **寄存器限制 (最常见):** Kernel 使用的寄存器过多，导致 SM 装不下理论最大数量的线程。
> 3. **共享内存限制:** Shared Memory 用得太多，SM 装不下更多 Block。

**Q18: 解释“性能悬崖” (Performance Cliff) 现象。**

> **A:** 指资源使用量（如寄存器）微小的增加，导致并行度断崖式下跌。
>
> - 例如：如果正好能跑 4 个 Block，但每个线程多用了 1 个寄存器导致总数超标，SM 就只能跑 3 个 Block。占用率瞬间从 100% 跌至 75%。

------

### 4.8 Querying Device Properties (设备查询)

**Q19: 为什么我们需要 `cudaGetDeviceProperties`？**

> **A:** 为了编写可移植的通用代码。不同显卡的 SM 数量、最大线程数、寄存器大小都不同。程序需要在运行时查询这些硬件属性，动态决定最佳的 Grid 大小和 Block 配置，避免在低端卡上崩溃或在高端卡上性能浪费。

**Q20: 即使 Block Size 可以是任意值，为什么通常推荐设为 128 或 256 这样的 32 的倍数？**

> **A:**
>
> 1. **Warp 对齐:** 避免产生包含大量无效线程的 Warp（Padding浪费）。
> 2. **硬件分割:** 硬件资源（如寄存器库）通常是以 Warp 为单位分配的，整倍数能更高效地利用资源。